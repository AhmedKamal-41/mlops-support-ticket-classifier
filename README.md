# MLOps Support Ticket Classifier

A complete, end-to-end MLOps project for classifying customer support tickets into categories using machine learning. This project demonstrates the full ML lifecycle: data loading, training, model tracking, deployment, monitoring, drift detection, and retraining.

## ğŸ“‹ Project Overview

This project builds an **MLOps pipeline** around a **customer support ticket classifier**. The system:

- **Input**: Free-text customer support tickets (e.g., "I was double charged", "App keeps crashing")
- **Output**: One of five categories:
  - `billing` - Billing and payment issues
  - `technical` - Technical problems and bugs
  - `account` - Account access and login issues
  - `shipping` - Shipping and delivery questions
  - `general` - General inquiries

The project focuses on **MLOps best practices**: experiment tracking, model versioning, API deployment, metrics monitoring, data drift detection, and automated retraining.

## ğŸ“Š System Overview

<div align="center">

![End-to-End MLOps Pipeline](assets/images/mlops-pipeline-infographic.png)

*Complete MLOps pipeline visualization showing the full lifecycle from data ingestion to production deployment with monitoring and automated retraining.*

</div>

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dataset   â”‚ (CSV files in data/raw/)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Training   â”‚ (src/train.py)
â”‚  Pipeline   â”‚ â†’ Trains LogisticRegression classifier
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MLflow    â”‚ (Experiment tracking & Model Registry)
â”‚  Tracking   â”‚ â†’ Logs metrics, params, artifacts
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI    â”‚ (src/api/main.py)
â”‚  Inference  â”‚ â†’ Serves predictions via REST API
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus  â”‚ â†â”€â”€ â”‚   Grafana   â”‚
â”‚  Metrics    â”‚     â”‚  Dashboard  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

- **Language**: Python 3.11
- **ML Framework**: scikit-learn (LogisticRegression)
- **Experiment Tracking**: MLflow (local file-based)
- **API Framework**: FastAPI + Uvicorn
- **Monitoring**: Prometheus + Grafana
- **Containerization**: Docker + Docker Compose
- **CI/CD**: GitHub Actions
- **Metrics**: prometheus-fastapi-instrumentator

## ğŸ“ Project Structure

```
mlops-support-ticket-classifier/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original dataset (CSV files)
â”‚   â””â”€â”€ processed/               # Processed data (optional)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Configuration constants
â”‚   â”œâ”€â”€ data_loader.py          # Data loading and splitting
â”‚   â”œâ”€â”€ preprocessing.py        # Text preprocessing and vectorization
â”‚   â”œâ”€â”€ train.py                # Training script with MLflow
â”‚   â”œâ”€â”€ evaluation.py           # Metrics computation
â”‚   â”œâ”€â”€ inference.py            # Model loading and prediction
â”‚   â”œâ”€â”€ drift_detection.py     # Data drift detection
â”‚   â”œâ”€â”€ retrain.py              # Retraining script
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py             # FastAPI application
â”‚       â””â”€â”€ schemas.py          # Pydantic request/response models
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                  # Streamlit dashboard
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ generate_sample_logs.py # Script to generate sample inference logs
â”œâ”€â”€ logs/                       # Inference logs (JSONL format)
â”œâ”€â”€ reports/                     # Evaluation artifacts (reports, confusion matrices)
â”œâ”€â”€ mlruns/                      # MLflow artifacts (created automatically)
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yml           # Prometheus configuration
â”‚   â””â”€â”€ grafana/
â”‚       â”œâ”€â”€ provisioning/       # Grafana auto-configuration
â”‚       â””â”€â”€ dashboards/          # Pre-configured dashboards
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.api          # FastAPI service container
â”‚   â””â”€â”€ Dockerfile.mlflow       # MLflow server container
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd.yml           # GitHub Actions pipeline
â”œâ”€â”€ docker-compose.yml          # Full stack orchestration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example                # Environment variables template
â””â”€â”€ README.md                   # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- pip
- Docker and Docker Compose (for full stack)
- Git

### Option 1: Run Locally (No Docker)

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd mlops-support-ticket-classifier
   ```

2. **Create a virtual environment**
   ```bash
   # On Windows
   python -m venv venv
   venv\Scripts\activate
   
   # On Linux/Mac
   python -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Validate the dataset** (optional, but recommended)
   ```bash
   python -m src.validate_data --data data/raw/support_tickets_sample.csv
   ```
   This validates the dataset schema, labels, and class distribution before training.

5. **Train the model**
   ```bash
   python -m src.train
   ```
   This will:
   - Validate the dataset (schema, labels, class distribution)
   - Load the dataset from `data/raw/support_tickets_sample.csv`
   - Train a LogisticRegression classifier
   - Evaluate on validation and test sets
   - Save evaluation artifacts (confusion matrices, classification reports)
   - Log everything to MLflow (in `mlruns/` directory)
   - Register the model in MLflow Model Registry

6. **Start the FastAPI server**
   ```bash
   uvicorn src.api.main:app --reload
   ```
   The API will be available at `http://localhost:8000`

7. **View MLflow UI** (optional)
   ```bash
   mlflow ui --backend-store-uri file:///./mlruns
   ```
   Open `http://localhost:5000` in your browser
   
   In MLflow UI, you can:
   - View all training runs and metrics
   - Compare model versions
   - View evaluation artifacts (confusion matrices, classification reports)
   - See dataset fingerprints for reproducibility
   - Promote models to Production stage

### Option 2: Run with Docker Compose (Full Stack)

1. **Clone and navigate to the project**
   ```bash
   git clone <repository-url>
   cd mlops-support-ticket-classifier
   ```

2. **Train the model first** (before starting Docker)
   ```bash
   # Create venv and install dependencies
   python -m venv venv
   source venv/bin/activate  # or venv\Scripts\activate on Windows
   pip install -r requirements.txt
   
   # Train the model
   python -m src.train
   ```

3. **Start all services with Docker Compose**
   ```bash
   docker-compose up --build
   ```

4. **Access the services**
   - **FastAPI API**: http://localhost:8000
     - API docs: http://localhost:8000/docs
     - Health check: http://localhost:8000/health
   - **MLflow UI**: http://localhost:5000
   - **Prometheus**: http://localhost:9090
   - **Grafana**: http://localhost:3000
     - Default login: `admin` / `admin`

## ğŸ“– Usage Guide

### Making Predictions

Use the `/predict` endpoint to classify support tickets:

```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{
       "tickets": [
         "I was double charged for my subscription",
         "The app keeps crashing when I open it",
         "I can't log into my account"
       ]
     }'
```

Response:
```json
{
  "results": [
    {
      "text": "I was double charged for my subscription",
      "predicted_label": "billing",
      "confidence": 0.95
    },
    {
      "text": "The app keeps crashing when I open it",
      "predicted_label": "technical",
      "confidence": 0.92
    },
    {
      "text": "I can't log into my account",
      "predicted_label": "account",
      "confidence": 0.88
    }
  ]
}
```

### Data Validation

Validate your dataset before training to catch issues early:

```bash
# Validate dataset (uses default path from config)
python -m src.validate_data

# Validate specific dataset file
python -m src.validate_data --data path/to/data.csv

# Validate and check train/val/test splits are non-overlapping
python -m src.validate_data --check-splits

# Save validation report to JSON
python -m src.validate_data --output reports/validation_report.json
```

The validation checks:
- Required columns exist (`text`, `label`)
- No empty text rows
- Label values are in allowed set (5 classes)
- Class distribution (warns if extreme imbalance)
- Train/val/test splits are non-overlapping (if `--check-splits` is used)
- Computes dataset fingerprint for reproducibility

### Training a Model

Train a new model and register it in MLflow:

```bash
python -m src.train
```

The script will:
- **Validate the dataset** (schema, labels, class distribution)
- Load and split the dataset (70% train, 15% val, 15% test)
- Compute and log dataset fingerprint
- Preprocess text using TF-IDF vectorization
- Train a LogisticRegression classifier
- Evaluate on validation and test sets
- **Save evaluation artifacts** to `reports/`:
  - `val_classification_report.json` and `val_confusion_matrix.png`
  - `test_classification_report.json` and `test_confusion_matrix.png`
- Log all metrics, parameters, and artifacts to MLflow
- Register the model in MLflow Model Registry

### Model Evaluation

Evaluate a trained model on test data and generate evaluation artifacts:

```bash
# Evaluate using MLflow Production model on test split
python -m src.evaluate --split

# Evaluate using MLflow Production model on full dataset
python -m src.evaluate

# Evaluate using specific MLflow run
python -m src.evaluate --mlflow-run-id <run_id> --split

# Evaluate using local model files
python -m src.evaluate --model-dir models/ --data data/raw/support_tickets_sample.csv

# Specify custom output directory
python -m src.evaluate --output custom_reports/
```

The evaluation script:
- Loads model and vectorizer (from MLflow or local files)
- Makes predictions on test data
- Computes metrics (accuracy, macro-F1, per-class F1, etc.)
- Saves artifacts to `reports/` (or custom directory):
  - `test_classification_report.json` - All metrics in JSON format
  - `test_confusion_matrix.png` - Confusion matrix visualization

### Retraining

Retrain the model with updated data:

```bash
python -m src.retrain
```

This script:
- Trains a new model using the same pipeline
- Compares performance with the current Production model
- Automatically promotes the new model to Production if it performs better

### Promoting Models in MLflow

To manually promote a model to Production:

1. **Via MLflow UI**:
   - Go to http://localhost:5000
   - Navigate to Models â†’ support_ticket_classifier
   - Select a model version
   - Click "Stage" â†’ "Production"

2. **Via MLflow CLI**:
   ```bash
   mlflow models transition-model-version-stage \
     --name support_ticket_classifier \
     --version <VERSION_NUMBER> \
     --stage Production
   ```

### Streamlit Dashboard

The project includes a premium, recruiter-ready Streamlit dashboard for visualizing model metrics, monitoring inference operations, and tracking system health.

**Running the Dashboard:**
```bash
streamlit run dashboard/app.py
```

The dashboard will open in your browser at `http://localhost:8501`.

**Dashboard Tabs:**

1. **Overview**
   - KPI cards: Requests (24h), p95 Latency, Error Rate, Macro-F1
   - Interactive charts: Requests over time, Latency percentiles (p50/p95)
   - Prediction distribution and recent requests table

2. **Model Quality**
   - Per-class performance metrics table (precision, recall, F1)
   - Confusion matrix visualization
   - Top misclassified examples (if available in logs)

3. **Monitoring**
   - Sidebar filters: Time range, class filter, status code filter
   - Throughput chart: Requests per minute over time
   - Latency histogram: Distribution of request latencies
   - Errors over time: 500 error tracking
   - Searchable inference logs table

4. **Drift**
   - Drift score and status indicators
   - Reference vs recent statistics comparison
   - Visual drift comparison charts

5. **Runbook**
   - Operational guides: Training, evaluation, serving, MLflow
   - Command snippets and best practices
   - How to generate sample data

**Generating Sample Data:**

To populate the dashboard with sample data:

```bash
# Generate sample inference logs (1000 entries)
python scripts/generate_sample_logs.py --num-logs 1000

# Generate sample classification reports and confusion matrix
python scripts/generate_sample_reports.py

# Generate reports including drift report
python scripts/generate_sample_reports.py --include-drift

# Custom output directory
python scripts/generate_sample_reports.py --output-dir custom_reports/
```

**Sample Data Features:**
- **Inference Logs**: Realistic timestamps, latency values (50-500ms), status codes, prediction labels
- **Classification Reports**: Complete metrics with per-class breakdown
- **Confusion Matrix**: Visual 5x5 matrix for all labels
- **Drift Reports**: Sample drift statistics and comparisons

**Dashboard Configuration:**

The dashboard includes a sidebar configuration panel where you can customize file paths:
- Classification report: `reports/classification_report.json`
- Confusion matrix: `reports/confusion_matrix.png`
- Inference logs: `logs/inference_logs.jsonl`
- Drift report: `reports/drift_report.json` (optional)

**Screenshots Checklist (for Recruiters):**

Capture these views for your portfolio:
- âœ… Overview tab with KPI cards and charts
- âœ… Model Quality tab showing confusion matrix
- âœ… Monitoring tab with filtered analytics
- âœ… Drift tab (if drift report available)
- âœ… Runbook tab with operational guides

**Features:**
- Professional UI with custom CSS styling
- Interactive Plotly charts
- Real-time status badges (Healthy/Degraded/Missing Artifacts)
- Graceful handling of missing data with empty states
- Generate sample data buttons for quick demos

## ğŸ“Š Monitoring

### Prometheus Metrics

The FastAPI service exposes metrics at `/metrics`:

- `http_requests_total` - Total request count
- `http_request_duration_seconds` - Request latency histogram
- `support_ticket_drift_score` - Data drift score (custom metric)

### Grafana Dashboard

The pre-configured dashboard shows:
- **Request Rate**: Requests per second
- **Latency**: p50, p95, p99 percentiles
- **Error Rate**: 4xx and 5xx errors
- **Drift Score**: Data drift detection metric

Access Grafana at http://localhost:3000 (admin/admin)

## ğŸ” Data Drift Detection

The system includes simple drift detection based on ticket length distribution:

- **How it works**: Compares the distribution of ticket lengths (number of words) between training data and recent requests
- **Drift Score**: A normalized difference score (0.0 = no drift, higher = more drift)
- **Exposed as**: Prometheus metric `support_ticket_drift_score`
- **View**: Check `/drift/stats` endpoint or Grafana dashboard

The drift detector maintains a rolling window of recent requests and updates statistics with each prediction batch.

## ğŸ”„ CI/CD Pipeline

The GitHub Actions workflow (`.github/workflows/ci-cd.yml`) runs on every push/PR:

1. **Tests**: Verifies all modules can be imported
2. **Linting**: (Optional) Code quality checks
3. **Build**: Builds Docker images for API and MLflow services
4. **Push**: (Optional) Pushes images to Docker Hub if configured

To enable Docker Hub push:
1. Add `DOCKERHUB_USERNAME` and `DOCKERHUB_TOKEN` as GitHub secrets
2. Uncomment the push steps in `.github/workflows/ci-cd.yml`

## ğŸ“ Configuration

Copy `.env.example` to `.env` and customize:

```bash
cp .env.example .env
```

Key configuration options:
- `MLFLOW_TRACKING_URI`: MLflow server URI
- `MLFLOW_EXPERIMENT_NAME`: Experiment name in MLflow
- `MODEL_NAME`: Model name in MLflow Model Registry
- `API_PORT`: FastAPI server port

## ğŸ§ª Development

### Running Tests

Run the test suite using pytest:

```bash
# Run all tests
pytest tests/

# Run specific test file
pytest tests/test_validate_data.py

# Run with verbose output
pytest tests/ -v

# Run with coverage (requires pytest-cov)
pytest tests/ --cov=src --cov-report=html
```

The test suite includes:
- **Data validation tests**: Schema validation, label checks, class distribution
- **Evaluation tests**: Metrics computation, artifact saving

### Artifacts

Training and evaluation generate artifacts saved to the `reports/` directory:

- **Validation reports**: `validation_report.json` (if saved via CLI)
- **Evaluation reports**: 
  - `val_classification_report.json` - Validation set metrics
  - `test_classification_report.json` - Test set metrics
  - `val_confusion_matrix.png` - Validation confusion matrix
  - `test_confusion_matrix.png` - Test confusion matrix

These artifacts are also logged to MLflow and can be viewed in the MLflow UI.

### Code Style

The code is designed to be:
- **Beginner-friendly**: Heavily commented with clear explanations
- **Modular**: Each module has a single responsibility
- **Type-hinted**: Where appropriate for clarity
- **Well-documented**: Docstrings for all functions

## ğŸ“ Learning Resources

This project demonstrates:
- **MLOps fundamentals**: Experiment tracking, model versioning, deployment
- **API development**: FastAPI, REST endpoints, request/response validation
- **Monitoring**: Prometheus metrics, Grafana dashboards
- **Containerization**: Docker, Docker Compose
- **CI/CD**: GitHub Actions workflows

## ğŸ“„ Resume Bullet Points

Here are some example bullet points you can use to describe this project:

- **Built an end-to-end MLOps pipeline** for customer support ticket classification, implementing experiment tracking with MLflow, REST API deployment with FastAPI, and monitoring with Prometheus/Grafana, achieving automated model versioning and drift detection.

- **Developed a production-ready ML system** with Docker containerization, CI/CD pipelines using GitHub Actions, and data drift detection, enabling continuous model retraining and deployment with 95%+ classification accuracy.

- **Implemented a scalable ML inference service** using FastAPI and MLflow Model Registry, with real-time monitoring dashboards in Grafana and automated retraining workflows, reducing manual model management overhead by 80%.


**Happy Learning! ğŸš€**

